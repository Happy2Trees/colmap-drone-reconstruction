import argparse
import json
import os
import random
import sys
from pathlib import Path
from typing import List, Tuple


IMG_EXTS = {".jpg", ".jpeg", ".png", ".bmp", ".tif", ".tiff"}


def _abs(p: Path) -> str:
    return str(p.resolve())


def find_dataset_root(preferred: str | None = None) -> Path:
    """Find a dataset root that contains images/ and labels/.

    Search order:
    - provided path (if any)
    - ./data/yolo
    - ../data/yolo
    - ./yolo_extracted
    """
    candidates = []
    if preferred:
        candidates.append(Path(preferred))
    candidates += [
        Path("data/yolo"),
        Path("../data/yolo"),
        Path("yolo_extracted"),
    ]
    for c in candidates:
        if (c / "images").exists() and (c / "labels").exists():
            return c.resolve()
    raise FileNotFoundError(
        "Dataset not found. Expected a folder with images/ and labels/. "
        "Tried: " + ", ".join(map(str, candidates))
    )


def collect_images(images_dir: Path) -> List[Path]:
    imgs = []
    for p in sorted(images_dir.glob("**/*")):
        if p.suffix.lower() in IMG_EXTS and p.is_file():
            imgs.append(p)
    return imgs


def ensure_splits(dataset_root: Path, split_dir: Path, val_ratio: float = 0.1, seed: int = 42) -> Tuple[Path, Path]:
    """Create train/val image lists (.txt) if a pre-made split is not found.

    Returns absolute paths to train.txt and val.txt.
    """
    images_root = dataset_root / "images"
    labels_root = dataset_root / "labels"

    # If COCO-like split directories exist, return them directly via list files for consistency
    split_train = images_root / "train"
    split_val = images_root / "val"
    if split_train.exists() and split_val.exists():
        train_imgs = collect_images(split_train)
        val_imgs = collect_images(split_val)
    else:
        # Flat images/labels; build a random split
        all_imgs = collect_images(images_root)
        # keep only those that have labels
        imgs = []
        for im in all_imgs:
            lab = labels_root / (im.stem + ".txt")
            if lab.exists():
                imgs.append(im)
        if not imgs:
            raise RuntimeError(f"No labeled images found under: {images_root}")
        random.Random(seed).shuffle(imgs)
        n_val = max(1, int(len(imgs) * val_ratio))
        val_imgs = imgs[:n_val]
        train_imgs = imgs[n_val:]
        if not train_imgs:
            # fall back to at least one train image
            train_imgs, val_imgs = imgs, imgs[: max(1, len(imgs)//10)]

    split_dir.mkdir(parents=True, exist_ok=True)
    train_list = split_dir / "train.txt"
    val_list = split_dir / "val.txt"
    train_list.write_text("\n".join(_abs(p) for p in train_imgs) + "\n", encoding="utf-8")
    val_list.write_text("\n".join(_abs(p) for p in val_imgs) + "\n", encoding="utf-8")
    return train_list.resolve(), val_list.resolve()


def write_pose_yaml(yaml_path: Path, train: Path, val: Path, class_name: str = "object", kpts: int = 1) -> Path:
    """Write a YOLO pose dataset YAML.

    Ultralytics expects:
    - train/val: dir paths or text files listing image paths
    - kpt_shape: [num_keypoints, 3]
    - names: list or dict of class names
    """
    yaml = f"""
# Auto-generated by src/detection/main.py
train: {train}
val: {val}
kpt_shape: [{kpts}, 3]
names: ["{class_name}"]
""".lstrip()
    yaml_path.parent.mkdir(parents=True, exist_ok=True)
    yaml_path.write_text(yaml, encoding="utf-8")
    return yaml_path.resolve()


def check_example_label(labels_root: Path, kpts: int = 1) -> None:
    """Light sanity check for label format: 1 class, bbox, and kpts*3 values.

    Each label line should be: cls cx cy w h x1 y1 v1 ... xK yK vK (normalized)
    """
    want_cols = 5 + 3 * kpts
    for f in sorted(labels_root.glob("**/*.txt")):
        txt = f.read_text(encoding="utf-8").strip()
        if not txt:
            continue
        line = txt.splitlines()[0].strip()
        parts = line.split()
        if len(parts) != want_cols:
            print(
                f"[warn] Label {f} has {len(parts)} columns, expected {want_cols} for {kpts} keypoint(s).",
                file=sys.stderr,
            )
        break


def do_train(args: argparse.Namespace) -> None:
    from ultralytics import YOLO

    data_root = find_dataset_root(args.data_root)
    check_example_label(data_root / "labels", kpts=1)

    # Ensure split file lists and dataset YAML
    splits_dir = Path(__file__).parent / "datasets" / "splits"
    train_list, val_list = ensure_splits(data_root, splits_dir, val_ratio=args.val_ratio, seed=args.seed)
    data_yaml = write_pose_yaml(
        yaml_path=Path(__file__).parent / "datasets" / "yolo_pose_data.yaml",
        train=train_list,
        val=val_list,
        class_name=args.class_name,
        kpts=1,
    )

    # Load model (pretrained pose) and train
    model = YOLO(args.model)

    # Strong augmentations for pose with 1 keypoint
    train_kwargs = dict(
        data=str(data_yaml),
        epochs=args.epochs,
        imgsz=args.imgsz,
        batch=args.batch,
        device=args.device,
        workers=args.workers,
        optimizer="auto",
        # augmentations: keep only the listed ones
        augment=True,
        degrees=10.0,
        translate=0.10,
        scale=0.60,
        shear=3.0,
        perspective=0.0005,
        fliplr=0.5,
        flipud=0.5,
        # explicitly disable other common augs
        mosaic=0.0,
        mixup=0.0,
        copy_paste=0.0,
        erasing=0.0,
        hsv_h=0.0,
        hsv_s=0.0,
        hsv_v=0.0,
        # Always start from pretrained pose weights if a YAML is provided.
        # If a .pt is provided to YOLO(), this flag is ignored safely.
        pretrained=True,
        project=str(Path("outputs") / "yolo_pose"),
        name=args.run_name,
        # Loss weights: lower box, higher pose
        box=5.0,
        pose=15.0,
        # box=5.0,
        # pose=15.0,
    )

    # Some options may vary by ultralytics version; run with a safe subset if needed
    try:
        model.train(**train_kwargs)
    except TypeError as e:
        print(f"[warn] Retrying train with reduced aug args due to: {e}")
        reduced = {k: v for k, v in train_kwargs.items() if k in {
            "data","epochs","imgsz","batch","device","workers","optimizer","augment",
            "degrees","translate","scale","shear","perspective","fliplr","flipud",
            # keep disables for widely supported args
            "mosaic","mixup","hsv_h","hsv_s","hsv_v",
            # ensure loss weights persist in fallback
            "box","pose",
            "project","name","pretrained"}}
        model.train(**reduced)


def do_predict(args: argparse.Namespace) -> None:
    from ultralytics import YOLO

    model = YOLO(args.weights)
    results = model.predict(
        source=args.source,
        imgsz=args.imgsz,
        conf=args.conf,
        iou=args.iou,
        device=args.device,
        save=args.save,
        project=str(Path("outputs") / "yolo_pose"),
        name=args.run_name if args.save else None,
    )
    # Example of accessing keypoints
    for i, r in enumerate(results):
        if getattr(r, "keypoints", None) is not None:
            print(f"[info] Image {i}: keypoints shape = {getattr(r.keypoints, 'shape', None)}")
            print(f"[info] First 5 keypoints (xy): {getattr(r.keypoints, 'xy', None)[:,:5] if hasattr(r.keypoints, 'xy') else None}")

    # Optional: save predictions to JSON files (per image)
    if getattr(args, "save_json", False):
        # Default JSON directory: outputs/yolo_pose/<run-name>/json
        base_dir = Path("outputs") / "yolo_pose" / args.run_name
        json_dir = Path(args.json_dir) if args.json_dir else (base_dir / "json")
        json_dir.mkdir(parents=True, exist_ok=True)

        for r in results:
            try:
                img_path = getattr(r, "path", None) or ""
                img_name = os.path.basename(img_path) if img_path else None
                h, w = getattr(r, "orig_shape", (None, None))
                names = getattr(r, "names", {}) or {}

                dets = []

                # Boxes
                n = 0
                xyxy = conf = cls_ids = None
                if getattr(r, "boxes", None) is not None and r.boxes is not None:
                    try:
                        xyxy = r.boxes.xyxy.detach().cpu().numpy().tolist()
                        conf = r.boxes.conf.detach().cpu().numpy().tolist() if getattr(r.boxes, 'conf', None) is not None else None
                        cls_ids = r.boxes.cls.detach().cpu().numpy().tolist() if getattr(r.boxes, 'cls', None) is not None else None
                        n = len(xyxy)
                    except Exception:
                        pass

                # Keypoints
                kpts_xy = kpts_conf = None
                if getattr(r, "keypoints", None) is not None and r.keypoints is not None:
                    try:
                        kpts_xy = r.keypoints.xy.detach().cpu().numpy().tolist()
                    except Exception:
                        kpts_xy = None
                    # confidence/visibility can be absent depending on version
                    try:
                        if hasattr(r.keypoints, 'conf') and r.keypoints.conf is not None:
                            kpts_conf = r.keypoints.conf.detach().cpu().numpy().tolist()
                        elif hasattr(r.keypoints, 'confidence') and r.keypoints.confidence is not None:
                            kpts_conf = r.keypoints.confidence.detach().cpu().numpy().tolist()
                    except Exception:
                        kpts_conf = None

                    if n == 0 and kpts_xy is not None:
                        n = len(kpts_xy)

                # Build detection list (align by index where possible)
                for i in range(n):
                    box_i = xyxy[i] if xyxy is not None and i < len(xyxy) else None
                    conf_i = conf[i] if conf is not None and i < len(conf) else None
                    cls_i = cls_ids[i] if cls_ids is not None and i < len(cls_ids) else None
                    label_i = names.get(int(cls_i)) if cls_i is not None and isinstance(names, dict) else None
                    kxy_i = kpts_xy[i] if kpts_xy is not None and i < len(kpts_xy) else None
                    kcf_i = kpts_conf[i] if kpts_conf is not None and i < len(kpts_conf) else None

                    dets.append({
                        "cls": int(cls_i) if cls_i is not None else None,
                        "label": label_i,
                        "conf": float(conf_i) if conf_i is not None else None,
                        "box_xyxy": [float(v) for v in box_i] if box_i is not None else None,
                        "keypoints_xy": [[float(x), float(y)] for x, y in kxy_i] if kxy_i is not None else None,
                        "keypoints_conf": [float(v) for v in kcf_i] if kcf_i is not None else None,
                    })

                record = {
                    "image": img_path,
                    "image_name": img_name,
                    "shape": [int(h) if h is not None else None, int(w) if w is not None else None],
                    "detections": dets,
                }

                out_path = json_dir / (Path(img_path).stem + ".json" if img_path else "prediction.json")
                with open(out_path, "w", encoding="utf-8") as f:
                    json.dump(record, f, ensure_ascii=False, indent=args.json_indent)
            except Exception as e:
                print(f"[warn] Failed to write JSON for {getattr(r, 'path', None)}: {e}")


def do_export(args: argparse.Namespace) -> None:
    from ultralytics import YOLO

    model = YOLO(args.weights)
    out = model.export(format=args.format, dynamic=args.dynamic, simplify=args.simplify, opset=args.opset)
    print(f"[info] Exported to: {out}")


def build_parser() -> argparse.ArgumentParser:
    p = argparse.ArgumentParser(
        description="Train/Predict/Export YOLO Pose on a custom dataset with 1 keypoint",
    )
    sub = p.add_subparsers(dest="cmd", required=True)

    # Train
    pt = sub.add_parser("train", help="Train a YOLO pose model")
    pt.add_argument("--data-root", default=None, help="Dataset root containing images/ and labels/. Defaults to data/yolo if present")
    pt.add_argument("--model", default="yolo11n-pose.pt", help="Base model weights or YAML")
    pt.add_argument("--epochs", type=int, default=100)
    pt.add_argument("--imgsz", type=int, default=640)
    pt.add_argument("--batch", type=int, default=16)
    pt.add_argument("--device", default=None, help="CUDA device like '0' or '0,1' or 'cpu'")
    pt.add_argument("--workers", type=int, default=8)
    pt.add_argument("--val-ratio", type=float, default=0.1, dest="val_ratio")
    pt.add_argument("--seed", type=int, default=42)
    pt.add_argument("--class-name", default="object")
    pt.add_argument("--run-name", default="train")
    pt.set_defaults(func=do_train)
    
    # Predict
    pp = sub.add_parser("predict", help="Run inference with a trained pose model")
    pp.add_argument("--weights", default="outputs/yolo_pose/train/weights/best.pt")
    pp.add_argument("--source", default="data/yolo/images")
    pp.add_argument("--imgsz", type=int, default=640)
    pp.add_argument("--conf", type=float, default=0.25)
    pp.add_argument("--iou", type=float, default=0.45)
    pp.add_argument("--device", default=None)
    pp.add_argument("--save", action="store_true")
    pp.add_argument("--run-name", default="predict")
    # JSON output controls
    pp.add_argument("--save-json", action="store_true", help="Save predictions (boxes/keypoints) to per-image JSON files")
    pp.add_argument("--json-dir", default=None, help="Directory to save JSON files. Defaults to outputs/yolo_pose/<run-name>/json")
    pp.add_argument("--json-indent", type=int, default=2, help="Indent level for JSON pretty-printing")
    pp.set_defaults(func=do_predict)

    # Export
    pe = sub.add_parser("export", help="Export a trained model to ONNX, etc.")
    pe.add_argument("--weights", default="outputs/yolo_pose/train/weights/best.pt")
    pe.add_argument("--format", default="onnx", choices=["onnx", "torchscript", "engine", "openvino", "coreml", "saved_model"])
    pe.add_argument("--dynamic", action="store_true")
    pe.add_argument("--simplify", action="store_true")
    pe.add_argument("--opset", type=int, default=None)
    pe.set_defaults(func=do_export)

    return p


def main(argv: List[str] | None = None) -> None:
    parser = build_parser()
    args = parser.parse_args(argv)
    args.func(args)


if __name__ == "__main__":
    main()
